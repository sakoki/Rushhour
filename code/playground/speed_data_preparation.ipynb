{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather speed data from SFMTA\n",
    "\n",
    "This notebook gathers data from the SFMTA website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, '../utils/')\n",
    "from api import get_sfmta_data\n",
    "from traffic_processing import coordinate_mapper\n",
    "from decorators import timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate_mapper = timer(coordinate_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get 2016 speed data\n",
    "This step can take a while depending on internet speed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url = 'ftp://avl-data.sfmta.com/avl_data/avl_raw/'\n",
    "# file_directory = '../../raw_data/sf_speed_data/'\n",
    "\n",
    "# get_sfmta_data(base_url, file_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean 2016 speed data\n",
    "\n",
    "*Note: Check that file path directories are properly configured*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dir = '../../raw_data/sf_speed_data/'\n",
    "# output_dir = '../../temp_data/sfdata_clean/'\n",
    "# Make sure .DS_Store is removed.\n",
    "\n",
    "# for file in os.listdir(input_dir):\n",
    "#     SFDATA_file_cleaner(input_dir, output_dir, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. load all data + format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered Census Zones (GIS data with GeoID)\n",
    "shp_file = '../../temp_data/sf_GEOID_GIS_data.shp'\n",
    "\n",
    "# Input directory\n",
    "input_dir = '../../temp_data/sfdata_clean/'\n",
    "\n",
    "# Output directory\n",
    "output_dir = '../../temp_data/sfdata_mapped/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load census data \n",
    "shp_file = gpd.GeoDataFrame.from_file(shp_file)\n",
    "print('Size of census zones df: {}'.format(shp_file.shape))\n",
    "shp_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All files by date\n",
    "for i, fname in enumerate(os.listdir(input_dir)):\n",
    "    print(i, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a. Map to coordinate regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all 29 files for the month of feburuary is there\n",
    "# Map each to corresponding census zones \n",
    "for fname in os.listdir(input_dir):\n",
    "    coordinate_mapper(shp_file, input_dir, output_dir, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b. Aggregate by census region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../../temp_data/sfdata_mapped/'\n",
    "output_dir ='../../temp_data/region_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_by_region(input_dir, output_dir):\n",
    "    \"\"\"Aggregate files by corresponding region ID (geoid10)\n",
    "    \n",
    "    For each file, the script will partition data by region ID.\n",
    "    Each region ID file will get updated every time a new file is read,\n",
    "    Each of the resulting files will contain all data pertaining to a region ID.\n",
    "    \n",
    "    :param str input_dir: directory containing input files\n",
    "    :param str output_dir: directory to save output files\n",
    "    :param str file_name: name of file \n",
    "    :return: table of all data for one region\n",
    "    :rtype: DataFrame \n",
    "    \"\"\"\n",
    "    \n",
    "    base_fname = 'time_series_region'\n",
    "    file_names = get_fname(input_dir, contains='2016')\n",
    "    \n",
    "    for file in file_names:\n",
    "        aggr_data = pd.read_csv(input_dir + file)\n",
    "        \n",
    "        # Loop though data for each region \n",
    "        # open only one region file at a time to save memory\n",
    "        for region_id, group_df in aggr_data.groupby('geoid10'):\n",
    "            # Check if region file already exists\n",
    "            output_fname = '{}/{}_{}.csv'.format(output_dir, base_fname, region_id)\n",
    "            if os.path.exists(output_fname):\n",
    "                f = open(output_fname,'a')\n",
    "            else:\n",
    "                f = open(output_fname,'w+')\n",
    "                f.write(','.join(aggr_data.columns)+'\\n')\n",
    "                \n",
    "            for i in group_df.itertuples():\n",
    "                f.write(','.join([str(cell) for cell in list(i)[1:]])+'\\n')\n",
    "                \n",
    "            f.close()\n",
    "\n",
    "        print('finished partitioning {}'.format(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_by_region(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir(output_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.c. Format data into frequency level timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../../temp_data/region_data/'\n",
    "print('Number of region files: {}'.format(len(os.listdir(input_dir))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_by_time_generator(path, columns=['REPORT_TIME'], Y='SPEED', unit='H', usecols=None):\n",
    "    \"\"\"Takes all regional time series data from a directory and aggregates them into one time series at desired time\n",
    "    frequency\n",
    "\n",
    "    Where the resulting DataFrame will contain the following columns:\n",
    "    +-----------+----+----+-----+----+\n",
    "    | region_ID | T1 | T2 | ... | TN |\n",
    "    +-----------+----+----+-----+----+\n",
    "\n",
    "    Each element in the columns T1...TN will be the averaged speed of all speeds recorded in a region at a specific time\n",
    "    point.\n",
    "\n",
    "    :param str path: input directory containing files of interest\n",
    "    :param list columns: name of column to be converted to datetime\n",
    "    :param str Y: name of column to be treated as the Y\n",
    "    :param unit: specification of time granularity\n",
    "    :param list usecols: specification of columns to read\n",
    "    :return: formatted table\n",
    "    :rtype: DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Reading files from directory: {}\".format(path))\n",
    "    file_names = get_fname(path, contains='')\n",
    "    new_time_df = pd.DataFrame()\n",
    "\n",
    "    for name in file_names:\n",
    "        region_data = pd.read_csv(path + name, parse_dates=columns, infer_datetime_format=True, usecols=usecols)\n",
    "        region_data.index = region_data[columns[0]]\n",
    "\n",
    "        # group second data into one time unit.\n",
    "        unit_aggregate = region_data[Y].resample(unit).mean()\n",
    "\n",
    "        # turn a series of data into a row(with dataframe type).\n",
    "        unit_aggregate = unit_aggregate.to_frame(name=re.sub(\"filtered_|time_series_|\\.csv\", \"\", name))\n",
    "        unit_aggregate = unit_aggregate.transpose()\n",
    "\n",
    "        # add into final result\n",
    "        new_time_df = pd.concat([new_time_df, unit_aggregate])\n",
    "\n",
    "    return new_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_time_series = region_by_time_generator(input_dir, columns=['REPORT_TIME'], Y='SPEED',unit='H', usecols=[1,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_time_series.to_csv('../../output/region_by_time_series.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
